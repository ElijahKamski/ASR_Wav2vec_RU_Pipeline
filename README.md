# ASR Wav2vec 2.0 RU Pipeline
## Here's some manual for launching Wav2Vec 2.0 by fairseq pretraining and finetuning on custom ASR Dataset

### First of all we need to setup faireq enviroment
```sh
git clone https://github.com/facebookresearch/fairseq
cd fairseq && pip install --editable ./
pip install soundfile
cd .. && git clone https://github.com/huggingface/transformers.git
cd ../fairseq
```
### Our next step is to generate train and valit tsv's for pretraining
We have to generate tsv files with two volumns. First will have path to our files (.wav) and second is to contain lenghth of audio
Here's the command for generating such files
```
python examples/wav2vec/wav2vec_manifest.py /path/to/waves --dest /manifest/path --ext $ext --valid-percent $valid
```
### Launch pretraining
Before this step we have to configure our model. All the settings can be found in config file. Example for my machine and dataset (RuDevices) is presented in this repo (`pretrain_base.yaml`)
So now we are ready to start the task (It will take a long time):
- task.data - path to folder with generated tsv's
- config-dir - path to folder with config
- config-name - name of a config in the folder
```sh
fairseq-hydra-train task.data={$path_to_train_folder} --config-dir=${path_to_configs_folder} --config-name=${config_name}
```
### Finetuning data 
At this point we need three files:
- dictionary generated by model above
- train.tsv
- train.ltr/train.wrd


Last files should be generated again. Unfortunately there is no ready way. Example of script for generation such files for RuDevices dataset could be found in this repo (`parse.py`)
There are several requirements for these files:
- default separator between tokens is '|' character
- transcription for each file should be written as sequence of tokens(word/letters) with specified separator in one line
- line number in .wrd/.ltr file should math with line number of the file in .tsv files

### Launching finetuning 
Plan of this step is similar to pretraining phase. We have to create a config file once again. Example for finetuning is also available (`pretrain_base.yaml`)
So now we are ready to start the task (It will take a long time):
- task.data - path to folder with generated tsv's, .wrd's/.ltr's and dict file
- config-dir - path to folder with config
- config-name - name of a config in the folder
- model.w2v_path - path to checkpoint of pretrained model
```sh
fairseq-hydra-train task.data={$path_to_train_folder} model.w2v_path=${model_path} --config-dir=${path_to_configs_folder} --config-name=${config_name}
```

## Last but not least
fairsesq CLI is not convinient enough to use this afterwards. So one important step is to convert model from pytorch checkpoint to HugingFace format (for this task we cloned transformers repo before). All we need to do is this one simple step:
``` python ../transformers/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py --pytorch_dump_folder_path ./outputs --checkpoint_path ./wav2vec_small_960h.pt --dict_path ./dict```
